// Smoldot
// Copyright (C) 2019-2022  Parity Technologies (UK) Ltd.
// SPDX-License-Identifier: GPL-3.0-or-later WITH Classpath-exception-2.0

// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU General Public License as published by
// the Free Software Foundation, either version 3 of the License, or
// (at your option) any later version.

// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU General Public License for more details.

// You should have received a copy of the GNU General Public License
// along with this program.  If not, see <http://www.gnu.org/licenses/>.

//! Background network service.
//!
//! The [`NetworkService`] manages background tasks dedicated to connecting to other nodes.
//! Importantly, its design is oriented towards the particular use case of the full node.
//!
//! The [`NetworkService`] spawns one background task (using the [`Config::tasks_executor`]) for
//! each active TCP socket, plus one for each TCP listening socket. Messages are exchanged between
//! the service and these background tasks.

// TODO: doc
// TODO: re-review this once finished

use crate::{database_thread, jaeger_service, LogCallback, LogLevel};

use core::{cmp, future::Future, mem, pin::Pin, task::Poll, time::Duration};
use futures_channel::oneshot;
use futures_lite::FutureExt as _;
use hashbrown::HashMap;
use smol::{
    channel, future,
    lock::Mutex,
    net::TcpStream,
    stream::{Stream, StreamExt as _},
};
use smoldot::{
    database::full_sqlite,
    header,
    informant::HashDisplay,
    libp2p::{
        connection,
        multiaddr::{self, Multiaddr, ProtocolRef},
        peer_id::{self, PeerId},
    },
    network::{basic_peering_strategy, protocol, service},
};
use std::{
    io,
    net::{IpAddr, SocketAddr},
    sync::Arc,
    time::Instant,
};

pub use smoldot::network::service::ChainId;

mod tasks;

/// Configuration for a [`NetworkService`].
pub struct Config {
    /// Closure that spawns background tasks.
    pub tasks_executor: Box<dyn FnMut(Pin<Box<dyn Future<Output = ()> + Send>>) + Send>,

    /// Function called in order to notify of something.
    pub log_callback: Arc<dyn LogCallback + Send + Sync>,

    /// Number of event receivers returned by [`NetworkService::new`].
    pub num_events_receivers: usize,

    /// Addresses to listen for incoming connections.
    pub listen_addresses: Vec<Multiaddr>,

    /// List of block chains to be connected to.
    pub chains: Vec<ChainConfig>,

    /// Value sent back for the agent version when receiving an identification request.
    pub identify_agent_version: String,

    /// Key used for the encryption layer.
    /// This is a Noise static key, according to the Noise specification.
    /// Signed using the actual libp2p key.
    pub noise_key: connection::NoiseKey,

    /// Service to use to report traces.
    pub jaeger_service: Arc<jaeger_service::JaegerService>,
}

/// Configuration for one chain.
pub struct ChainConfig {
    /// Name of the chain to use for logging purposes.
    pub log_name: String,

    /// List of node identities and addresses that are known to belong to the chain's peer-to-pee
    /// network.
    pub bootstrap_nodes: Vec<(PeerId, Multiaddr)>,

    /// Database to use to read blocks from when answering requests.
    pub database: Arc<database_thread::DatabaseThread>,

    /// Hash of the genesis block of the chain. Sent to other nodes in order to determine whether
    /// the chains match.
    pub genesis_block_hash: [u8; 32],

    /// Number and hash of the current best block. Can later be updated with // TODO: which function?
    pub best_block: (u64, [u8; 32]),

    /// Optional identifier to insert into the networking protocol names. Used to differentiate
    /// between chains with the same genesis hash.
    pub fork_id: Option<String>,

    /// Number of bytes of the block number in the networking protocol.
    pub block_number_bytes: usize,

    /// Must be `Some` if and only if the chain uses the GrandPa networking protocol. Contains the
    /// number of the finalized block at the time of the initialization.
    pub grandpa_protocol_finalized_block_height: Option<u64>,
}

/// Event generated by the events reporters returned by [`NetworkService::new`].
#[derive(Debug, Clone)]
pub enum Event {
    Connected {
        chain_id: ChainId,
        peer_id: PeerId,
        best_block_number: u64,
        best_block_hash: [u8; 32],
    },
    Disconnected {
        chain_id: ChainId,
        peer_id: PeerId,
    },
    BlockAnnounce {
        chain_id: ChainId,
        peer_id: PeerId,
        scale_encoded_header: Vec<u8>,
        is_best: bool,
    },
}

pub struct NetworkService {
    /// Identity of the local node.
    local_peer_id: PeerId,

    /// Service to use to report traces.
    jaeger_service: Arc<jaeger_service::JaegerService>,

    /// Channel to send messages to the background task.
    to_background_tx: Mutex<channel::Sender<ToBackground>>,

    /// Name of all the chains that have been registered, for logging purposes.
    chain_names: hashbrown::HashMap<ChainId, String, fnv::FnvBuildHasher>,

    /// See [`Config::log_callback`].
    log_callback: Arc<dyn LogCallback + Send + Sync>,

    /// Notified when the service shuts down.
    foreground_shutdown: event_listener::Event,
}

enum ToBackground {
    FromConnectionTask {
        connection_id: service::ConnectionId,
        opaque_message: Option<service::ConnectionToCoordinator>,
        connection_now_dead: bool,
    },
    IncomingConnection {
        socket: TcpStream,
        multiaddr: Multiaddr,
        when_accepted: Instant,
    },
    StartKademliaDiscoveries {
        when_done: oneshot::Sender<()>,
    },
    ForegroundAnnounceBlock {
        target: PeerId,
        chain_id: ChainId,
        scale_encoded_header: Vec<u8>,
        is_best: bool,
        result_tx: oneshot::Sender<Result<(), service::QueueNotificationError>>,
    },
    ForegroundSetLocalBestBlock {
        chain_id: ChainId,
        best_hash: [u8; 32],
        best_number: u64,
    },
    ForegroundBlocksRequest {
        target: PeerId,
        chain_id: ChainId,
        config: protocol::BlocksRequestConfig,
        result_tx: oneshot::Sender<Result<Vec<protocol::BlockData>, BlocksRequestError>>,
    },
    ForegroundGetNumConnections {
        result_tx: oneshot::Sender<usize>,
    },
    ForegroundGetNumPeers {
        chain_id: ChainId,
        result_tx: oneshot::Sender<usize>,
    },
    ForegroundGetNumTotalPeers {
        result_tx: oneshot::Sender<usize>,
    },
    ForegroundShutdown,
}
struct Inner {
    /// Value provided through [`Config::identify_agent_version`].
    identify_agent_version: String,

    /// Sending events through the public API.
    ///
    /// Contains either senders, or a `Future` that is currently sending an event and will yield
    /// the senders back once it is finished.
    event_senders: either::Either<
        Vec<channel::Sender<Event>>,
        Pin<Box<dyn Future<Output = Vec<channel::Sender<Event>>> + Send>>,
    >,

    /// Identity of the local node.
    local_peer_id: PeerId,

    /// Service to use to report traces.
    jaeger_service: Arc<jaeger_service::JaegerService>,

    /// Data structure holding the entire state of the networking.
    network: service::ChainNetwork<Instant>,

    // TODO: should be a user data in `ChainNetwork`
    chains: hashbrown::HashMap<ChainId, Chain, fnv::FnvBuildHasher>,

    /// Data structure holding the addresses and assigned slots.
    peering_strategy: basic_peering_strategy::BasicPeeringStrategy<ChainId, Instant>,

    /// Current number of outgoing connection attempts.
    ///
    /// This counter is used to limit the number of simultaneous connection attempts, as some
    /// ISPs/cloud providers don't like seeing too many dialing connections at the same time.
    num_pending_out_attempts: usize,

    /// See [`Config::tasks_executor`].
    tasks_executor: Box<dyn FnMut(Pin<Box<dyn Future<Output = ()> + Send>>) + Send>,

    /// See [`Config::log_callback`].
    log_callback: Arc<dyn LogCallback + Send + Sync>,

    active_connections: HashMap<
        service::ConnectionId,
        channel::Sender<service::CoordinatorToConnection>,
        fnv::FnvBuildHasher,
    >,

    process_network_service_events: bool,

    /// Channel for the various tasks to send messages to the background task.
    to_background_rx: channel::Receiver<ToBackground>,

    /// Sending side of [`Inner::to_background_rx`].
    to_background_tx: channel::Sender<ToBackground>,

    /// List of all block requests that have been started but not finished yet.
    blocks_requests: HashMap<
        service::SubstreamId,
        oneshot::Sender<Result<Vec<protocol::BlockData>, BlocksRequestError>>,
        fnv::FnvBuildHasher,
    >,

    /// List of Kademlia discovery operations that have been started but not finished yet.
    kademlia_find_nodes_requests: HashMap<service::SubstreamId, ChainId, fnv::FnvBuildHasher>,
}

/// Extra information of a chain.
struct Chain {
    /// Name of the chain to use for logging purposes.
    log_name: String,

    /// How to access data to answer requests from the remotes.
    database: Arc<database_thread::DatabaseThread>,
}

impl NetworkService {
    /// Initializes the network service with the given configuration.
    pub async fn new(
        config: Config,
    ) -> Result<
        (
            Arc<Self>,
            Vec<ChainId>,
            Vec<Pin<Box<dyn Stream<Item = Event> + Send>>>,
        ),
        InitError,
    > {
        let (event_senders, event_receivers): (Vec<_>, Vec<_>) = (0..config.num_events_receivers)
            .map(|_| channel::bounded(16))
            .unzip();

        let mut network = service::ChainNetwork::new(service::Config {
            chains_capacity: config.chains.len(),
            connections_capacity: 100, // TODO: ?
            noise_key: config.noise_key,
            handshake_timeout: Duration::from_secs(8),
            randomness_seed: rand::random(),
        });

        let mut peering_strategy = basic_peering_strategy::BasicPeeringStrategy::new();

        let mut chains =
            hashbrown::HashMap::with_capacity_and_hasher(config.chains.len(), Default::default());
        let mut chain_names =
            hashbrown::HashMap::with_capacity_and_hasher(config.chains.len(), Default::default());

        for chain in config.chains {
            let chain_id = network
                .add_chain(service::ChainConfig {
                    fork_id: chain.fork_id.clone(),
                    block_number_bytes: chain.block_number_bytes,
                    best_hash: chain.best_block.1,
                    best_number: chain.best_block.0,
                    genesis_hash: chain.genesis_block_hash,
                    role: protocol::Role::Full,
                    grandpa_protocol_config: if let Some(commit_finalized_height) =
                        chain.grandpa_protocol_finalized_block_height
                    {
                        // TODO: dummy values
                        Some(service::GrandpaState {
                            commit_finalized_height,
                            round_number: 1,
                            set_id: 0,
                        })
                    } else {
                        None
                    },
                    allow_inbound_block_requests: true,
                })
                .unwrap(); // TODO: don't unwrap?

            for (peer_id, addr) in chain.bootstrap_nodes {
                peering_strategy.insert_address(&peer_id, addr.into_vec());
                peering_strategy.insert_chain_peer(chain_id, peer_id);
            }

            chain_names.insert(chain_id, chain.log_name.clone());

            chains.insert(
                chain_id,
                Chain {
                    log_name: chain.log_name,
                    database: chain.database,
                },
            );
        }

        let (to_background_tx, to_background_rx) = channel::bounded(64);
        let foreground_shutdown = event_listener::Event::new();

        let local_peer_id =
            peer_id::PublicKey::Ed25519(*network.noise_key().libp2p_public_ed25519_key())
                .into_peer_id();

        // Initialize the inner network service.
        let mut inner = Inner {
            local_peer_id: local_peer_id.clone(),
            identify_agent_version: config.identify_agent_version,
            event_senders: either::Left(event_senders),
            chains,
            num_pending_out_attempts: 0,
            to_background_rx,
            to_background_tx: to_background_tx.clone(),
            process_network_service_events: true,
            tasks_executor: config.tasks_executor,
            log_callback: config.log_callback.clone(),
            network,
            peering_strategy,
            active_connections: hashbrown::HashMap::with_capacity_and_hasher(
                100, // TODO: ?
                Default::default(),
            ),
            blocks_requests: hashbrown::HashMap::with_capacity_and_hasher(
                50, // TODO: ?
                Default::default(),
            ),
            kademlia_find_nodes_requests: hashbrown::HashMap::with_capacity_and_hasher(
                4,
                Default::default(),
            ),
            jaeger_service: config.jaeger_service.clone(),
        };

        // For each listening address in the configuration, create a background task dedicated to
        // listening on that address.
        for listen_address in config.listen_addresses {
            // Try to parse the requested address and create the corresponding listening socket.
            let tcp_listener: smol::net::TcpListener = {
                let addr = {
                    let mut iter = listen_address.iter();
                    let proto1 = iter.next();
                    let proto2 = iter.next();
                    let proto3 = iter.next();
                    match (proto1, proto2, proto3) {
                        (Some(ProtocolRef::Ip4(ip)), Some(ProtocolRef::Tcp(port)), None) => {
                            Some(SocketAddr::from((ip, port)))
                        }
                        (Some(ProtocolRef::Ip6(ip)), Some(ProtocolRef::Tcp(port)), None) => {
                            Some(SocketAddr::from((ip, port)))
                        }
                        _ => None,
                    }
                };

                if let Some(addr) = addr {
                    match smol::net::TcpListener::bind(addr).await {
                        Ok(l) => l,
                        Err(err) => {
                            return Err(InitError::ListenerIo(listen_address, err));
                        }
                    }
                } else {
                    // TODO: support WebSocket server
                    return Err(InitError::BadListenMultiaddr(listen_address));
                }
            };

            // Spawn a background task dedicated to this listener.
            (inner.tasks_executor)(Box::pin({
                let to_background_tx = to_background_tx.clone();
                let log_callback = config.log_callback.clone();
                let mut on_foreground_shutdown = foreground_shutdown.listen();
                async move {
                    loop {
                        let Some(accept_result) = future::or(
                            async {
                                (&mut on_foreground_shutdown).await;
                                None
                            },
                            async { Some(tcp_listener.accept().await) },
                        )
                        .await
                        else {
                            break;
                        };

                        let when_accepted = Instant::now();

                        let (socket, addr) = match accept_result {
                            Ok(v) => v,
                            Err(error) => {
                                // Errors here can happen if the accept failed, for example if no
                                // file descriptor is available.
                                // A wait is added in order to avoid having a busy-loop failing to
                                // accept connections.
                                log_callback.log(
                                    LogLevel::Warn,
                                    format!("tcp-accept-error; error={}", error),
                                );
                                smol::Timer::after(Duration::from_secs(2)).await;
                                continue;
                            }
                        };

                        // The Nagle algorithm, implemented in the kernel, consists in buffering the
                        // data to be sent out and waiting a bit before actually sending it out, in
                        // order to potentially merge multiple writes in a row into one packet. In
                        // the implementation below, it is guaranteed that the buffer in `WithBuffers`
                        // is filled with as much data as possible before the operating system gets
                        // involved. As such, we disable the Nagle algorithm, in order to avoid adding
                        // an artificial delay to all sends.
                        let _ = socket.set_nodelay(true);

                        let multiaddr = [
                            match addr.ip() {
                                IpAddr::V4(ip) => ProtocolRef::Ip4(ip.octets()),
                                IpAddr::V6(ip) => ProtocolRef::Ip6(ip.octets()),
                            },
                            ProtocolRef::Tcp(addr.port()),
                        ]
                        .into_iter()
                        .collect::<Multiaddr>();

                        log_callback.log(
                            LogLevel::Debug,
                            format!("incoming-connection; multiaddr={}", multiaddr),
                        );

                        let _ = to_background_tx
                            .send(ToBackground::IncomingConnection {
                                socket,
                                multiaddr,
                                when_accepted,
                            })
                            .await;
                    }
                }
            }))
        }

        // Spawn a task that sends a "shutdown" message whenever the service shuts down.
        (inner.tasks_executor)({
            let on_foreground_shutdown = foreground_shutdown.listen();
            let to_background_tx = to_background_tx.clone();
            Box::pin(async move {
                let () = on_foreground_shutdown.await;
                let _ = to_background_tx
                    .send(ToBackground::ForegroundShutdown)
                    .await;
            })
        });

        // Spawn task starts a discovery request at a periodic interval.
        // This is done through a separate task due to ease of implementation.
        (inner.tasks_executor)(Box::pin({
            let to_background_tx = to_background_tx.clone();
            let mut on_foreground_shutdown = foreground_shutdown.listen();
            async move {
                let mut next_discovery = Duration::from_secs(1);

                loop {
                    let still_alive = future::race(
                        async {
                            smol::Timer::after(next_discovery).await;
                            true
                        },
                        async {
                            (&mut on_foreground_shutdown).await;
                            false
                        },
                    )
                    .await;
                    if !still_alive {
                        break;
                    }

                    next_discovery = cmp::min(next_discovery * 2, Duration::from_secs(120));
                    let (when_done, when_done_rx) = oneshot::channel();
                    let _ = to_background_tx
                        .send(ToBackground::StartKademliaDiscoveries { when_done })
                        .await;
                    let _ = when_done_rx.await;
                }
            }
        }));

        // Build the final network service.
        let network_service = Arc::new(NetworkService {
            local_peer_id,
            chain_names,
            jaeger_service: config.jaeger_service,
            to_background_tx: Mutex::new(to_background_tx),
            log_callback: config.log_callback,
            foreground_shutdown,
        });

        // Spawn the main task dedicated to processing the network.
        run(inner);

        // Adjust the receivers to keep the `network_service` alive.
        // TODO: no, hacky
        let receivers = event_receivers
            .into_iter()
            .map(|rx| {
                let mut network_service = Some(network_service.clone());
                rx.chain(smol::stream::poll_fn(move |_| {
                    drop(network_service.take());
                    Poll::Ready(None)
                }))
                .boxed()
            })
            .collect();

        let chain_ids = network_service.chain_names.keys().cloned().collect();
        Ok((network_service, chain_ids, receivers))
    }

    /// Returns the peer ID of the local node.
    pub fn local_peer_id(&self) -> &PeerId {
        &self.local_peer_id
    }

    /// Returns the number of connections, both handshaking or established, both incoming and
    /// outgoing.
    pub async fn num_connections(&self) -> usize {
        let (result_tx, result_rx) = oneshot::channel();

        let _ = self
            .to_background_tx
            .lock()
            .await
            .send(ToBackground::ForegroundGetNumConnections { result_tx })
            .await;

        result_rx.await.unwrap()
    }

    /// Returns the number of peers we have a substream with,.
    pub async fn num_peers(&self, chain_id: ChainId) -> usize {
        let (result_tx, result_rx) = oneshot::channel();

        let _ = self
            .to_background_tx
            .lock()
            .await
            .send(ToBackground::ForegroundGetNumPeers {
                chain_id,
                result_tx,
            })
            .await;

        result_rx.await.unwrap()
    }

    /// Returns the number of peers we have a substream with, all chains added together.
    pub async fn num_total_peers(&self) -> usize {
        let (result_tx, result_rx) = oneshot::channel();

        let _ = self
            .to_background_tx
            .lock()
            .await
            .send(ToBackground::ForegroundGetNumTotalPeers { result_tx })
            .await;

        result_rx.await.unwrap()
    }

    pub async fn set_local_best_block(
        &self,
        chain_id: ChainId,
        best_hash: [u8; 32],
        best_number: u64,
    ) {
        let _ = self
            .to_background_tx
            .lock()
            .await
            .send(ToBackground::ForegroundSetLocalBestBlock {
                chain_id,
                best_hash,
                best_number,
            })
            .await;
    }

    pub async fn send_block_announce(
        self: Arc<Self>,
        target: PeerId,
        chain_id: ChainId,
        scale_encoded_header: Vec<u8>,
        is_best: bool,
    ) -> Result<(), service::QueueNotificationError> {
        let (result_tx, result_rx) = oneshot::channel();

        let _ = self
            .to_background_tx
            .lock()
            .await
            .send(ToBackground::ForegroundAnnounceBlock {
                target,
                chain_id,
                scale_encoded_header,
                is_best,
                result_tx,
            })
            .await;

        result_rx.await.unwrap()
    }

    /// Sends a blocks request to the given peer.
    // TODO: more docs
    // TODO: proper error type
    pub async fn blocks_request(
        self: Arc<Self>,
        target: PeerId, // TODO: by value?
        chain_id: ChainId,
        config: protocol::BlocksRequestConfig,
    ) -> Result<Vec<protocol::BlockData>, BlocksRequestError> {
        let chain_name = self.chain_names[&chain_id].clone();

        self.log_callback.log(
            LogLevel::Debug,
            format!(
                "blocks-request-start; peer_id={}; chain={}; start={}; desired_count={}; direction={}",
                target,
                chain_name,
                match &config.start {
                    protocol::BlocksRequestConfigStart::Hash(h) => either::Left(HashDisplay(h)),
                    protocol::BlocksRequestConfigStart::Number(n) => either::Right(n),
                },
                config.desired_count,
                match config.direction {
                    protocol::BlocksRequestDirection::Ascending => "ascending",
                    protocol::BlocksRequestDirection::Descending => "descending",
                },
            ),
        );

        // Setup a guard that will print a log message in case it is dropped silently.
        // This lets us detect if the request is cancelled.
        struct LogIfCancel(PeerId, String, Arc<dyn LogCallback + Send + Sync>);
        impl Drop for LogIfCancel {
            fn drop(&mut self) {
                self.2.log(
                    LogLevel::Debug,
                    format!(
                        "blocks-request-ended; peer_id={}; chain={}; outcome=cancelled",
                        self.0, self.1
                    ),
                );
            }
        }
        let _log_if_cancel = LogIfCancel(
            target.clone(),
            chain_name.clone(),
            self.log_callback.clone(),
        );

        let _jaeger_span = self.jaeger_service.outgoing_block_request_span(
            &self.local_peer_id,
            &target,
            config.desired_count.get(),
            if let (1, protocol::BlocksRequestConfigStart::Hash(block_hash)) =
                (config.desired_count.get(), &config.start)
            {
                Some(block_hash)
            } else {
                None
            },
        );

        let (result_tx, result_rx) = oneshot::channel();

        let _ = self
            .to_background_tx
            .lock()
            .await
            .send(ToBackground::ForegroundBlocksRequest {
                target: target.clone(),
                chain_id,
                config,
                result_tx,
            })
            .await;

        let result = result_rx.await.unwrap();

        // Requet has finished. Print the log and prevent the cancellation message from being
        // printed.
        mem::forget(_log_if_cancel);
        match &result {
            Ok(success) => {
                self.log_callback.log(LogLevel::Debug, format!(
                    "blocks-request-ended; peer_id={}; chain={}; outcome=success; response_blocks={}",
                    target, chain_name, success.len()
                ));
            }
            Err(err) => {
                self.log_callback.log(
                    LogLevel::Debug,
                    format!(
                        "blocks-request-ended; peer_id={}; chain={}; outcome=failure; error={}",
                        target, chain_name, err
                    ),
                );
            }
        }

        result
    }
}

impl Drop for NetworkService {
    fn drop(&mut self) {
        self.foreground_shutdown.notify(usize::max_value());
    }
}

/// Error when initializing the network service.
#[derive(Debug, derive_more::Display)]
pub enum InitError {
    /// I/O error when initializing a listener.
    #[display(fmt = "I/O error when creating listener for {_0}: {_1}")]
    ListenerIo(Multiaddr, io::Error),
    /// A listening address passed through the configuration isn't valid.
    #[display(fmt = "A listening address passed through the configuration isn't valid: {_0}")]
    BadListenMultiaddr(Multiaddr),
}

/// Error returned by [`NetworkService::blocks_request`].
#[derive(Debug, derive_more::Display)]
pub enum BlocksRequestError {
    /// No established connection with the target.
    NoConnection,
    /// Error during the request.
    #[display(fmt = "{_0}")]
    Request(service::BlocksRequestError),
}

fn run(mut inner: Inner) {
    // This function is a small hack because I didn't find a better way to store the executor
    // within `Inner` while at the same time spawning the `Inner` using said executor.
    let mut actual_executor = mem::replace(&mut inner.tasks_executor, Box::new(|_| unreachable!()));
    let (tx, rx) = oneshot::channel();
    actual_executor(Box::pin(async move {
        let actual_executor = rx.await.unwrap();
        inner.tasks_executor = actual_executor;
        background_task(inner).await;
    }));
    tx.send(actual_executor).unwrap_or_else(|_| panic!());
}

async fn background_task(mut inner: Inner) {
    loop {
        // Pull messages that the coordinator has generated in destination to the various
        // connections.
        while let Some((connection_id, message)) = inner.network.pull_message_to_connection() {
            // Note that it is critical for the sending to not take too long here, in order to not
            // block the process of the network service.
            // In particular, if sending the message to the connection is blocked due to sending
            // a message on the connection-to-coordinator channel, this will result in a deadlock.
            // For this reason, the connection task is always ready to immediately accept a message
            // on the coordinator-to-connection channel.
            inner
                .active_connections
                .get_mut(&connection_id)
                .unwrap()
                .send(message)
                .await
                .unwrap();
        }

        if inner.process_network_service_events && matches!(inner.event_senders, either::Left(_)) {
            let event = loop {
                let inner_event = match inner.network.next_event() {
                    Some(ev) => ev,
                    None => break None,
                };

                match inner_event {
                    service::Event::HandshakeFinished {
                        id,
                        expected_peer_id,
                        peer_id,
                        ..
                    } => {
                        inner.num_pending_out_attempts -= 1;

                        let remote_addr = Multiaddr::try_from(
                            inner.network.connection_remote_addr(id).to_owned(),
                        )
                        .unwrap(); // TODO: review this unwrap
                        if let Some(expected_peer_id) =
                            expected_peer_id.as_ref().filter(|p| **p != peer_id)
                        {
                            inner
                            .log_callback
                            .log(LogLevel::Debug, format!("connected-peer-id-mismatch; expected_peer_id={}; actual_peer_id={}; address={}", expected_peer_id, peer_id, remote_addr));

                            inner
                                .peering_strategy
                                .remove_address(expected_peer_id, remote_addr.as_ref());
                            inner
                                .peering_strategy
                                .insert_connected_address(&peer_id, remote_addr.clone().into_vec());
                        } else {
                            inner
                                .log_callback
                                .log(LogLevel::Debug, format!("connected; peer_id={}", peer_id));
                        }
                    }
                    service::Event::PreHandshakeDisconnected {
                        address,
                        expected_peer_id,
                        ..
                    } => {
                        inner.num_pending_out_attempts -= 1;
                        if let Some(expected_peer_id) = expected_peer_id {
                            inner
                                .peering_strategy
                                .disconnect_addr(&expected_peer_id, &address)
                                .unwrap();
                            let address = Multiaddr::try_from(address).unwrap();
                            inner.log_callback.log(
                                LogLevel::Debug,
                                format!(
                                    "disconnected; handshake-finished=false; peer_id={}; address={}",
                                    expected_peer_id, address
                                ),
                            );
                        }
                    }
                    service::Event::Disconnected {
                        address, peer_id, ..
                    } => {
                        inner
                            .peering_strategy
                            .disconnect_addr(&peer_id, &address)
                            .unwrap();
                        let address = Multiaddr::try_from(address).unwrap();
                        inner.log_callback.log(
                            LogLevel::Debug,
                            format!(
                                "disconnected; handshake-finished=true; peer_id={}; address={}",
                                peer_id, address
                            ),
                        );
                    }
                    service::Event::BlockAnnounce {
                        chain_id,
                        peer_id,
                        announce,
                    } => {
                        let decoded = announce.decode();
                        let header_hash =
                            header::hash_from_scale_encoded_header(decoded.scale_encoded_header);
                        match header::decode(
                            decoded.scale_encoded_header,
                            inner.network.block_number_bytes(chain_id),
                        ) {
                            Ok(decoded_header) => {
                                let mut _jaeger_span =
                                    inner.jaeger_service.block_announce_receive_span(
                                        &inner.local_peer_id,
                                        &peer_id,
                                        decoded_header.number,
                                        &decoded_header
                                            .hash(inner.network.block_number_bytes(chain_id)),
                                    );

                                inner.log_callback.log(LogLevel::Debug, format!(
                                    "block-announce; peer_id={}; chain={}; hash={}; number={}; is_best={:?}",
                                    peer_id, inner.chains[&chain_id].log_name, HashDisplay(&header_hash), decoded_header.number, decoded.is_best
                                ));

                                break Some(Event::BlockAnnounce {
                                    chain_id,
                                    peer_id,
                                    is_best: decoded.is_best,
                                    scale_encoded_header: decoded.scale_encoded_header.to_owned(), // TODO: somewhat wasteful to copy here, could pass the entire announce
                                });
                            }
                            Err(error) => {
                                inner.log_callback.log(LogLevel::Warn, format!(
                                    "block-announce-bad-header; peer_id={}; chain={}; hash={}; is_best={:?}; error={}",
                                    peer_id, inner.chains[&chain_id].log_name, HashDisplay(&header_hash), decoded.is_best, error
                                ));

                                if inner.network.gossip_remove_desired(
                                    chain_id,
                                    &peer_id,
                                    service::GossipKind::ConsensusTransactions,
                                ) {
                                    inner.peering_strategy.unassign_slot_and_ban(
                                        &chain_id,
                                        &peer_id,
                                        Instant::now() + Duration::from_secs(10),
                                    );
                                    inner.log_callback.log(
                                        LogLevel::Debug,
                                        format!(
                                            "slot-unassigned; peer_id={}; chain={}",
                                            peer_id, inner.chains[&chain_id].log_name
                                        ),
                                    );
                                }
                                let _ = inner.network.gossip_close(
                                    chain_id,
                                    &peer_id,
                                    service::GossipKind::ConsensusTransactions,
                                ); // TODO: what is the return value?

                                inner.process_network_service_events = true;
                            }
                        }
                    }
                    service::Event::GossipConnected {
                        peer_id,
                        chain_id,
                        best_number,
                        best_hash,
                        ..
                    } => {
                        inner.log_callback.log(
                            LogLevel::Debug,
                            format!(
                            "chain-connected; peer_id={}; chain={}; best_number={}; best_hash={}",
                            peer_id,
                            inner.chains[&chain_id].log_name,
                            best_number,
                            HashDisplay(&best_hash),
                        ),
                        );
                        break Some(Event::Connected {
                            peer_id,
                            chain_id,
                            best_block_number: best_number,
                            best_block_hash: best_hash,
                        });
                    }
                    service::Event::GossipDisconnected {
                        peer_id, chain_id, ..
                    } => {
                        inner.log_callback.log(
                            LogLevel::Debug,
                            format!(
                                "chain-disconnected; peer_id={}; chain={}",
                                peer_id, inner.chains[&chain_id].log_name
                            ),
                        );

                        // Note that peer doesn't necessarily have an out slot, as this event
                        // might happen as a result of an inbound gossip connection.
                        inner.peering_strategy.unassign_slot_and_ban(
                            &chain_id,
                            &peer_id,
                            Instant::now() + Duration::from_secs(10),
                        );
                        if inner.network.gossip_remove_desired(
                            chain_id,
                            &peer_id,
                            service::GossipKind::ConsensusTransactions,
                        ) {
                            inner.log_callback.log(
                                LogLevel::Debug,
                                format!(
                                    "slot-unassigned; peer_id={}; chain={}",
                                    peer_id, inner.chains[&chain_id].log_name
                                ),
                            );
                        }

                        inner.process_network_service_events = true;

                        break Some(Event::Disconnected { chain_id, peer_id });
                    }
                    service::Event::GossipOpenFailed {
                        chain_id,
                        peer_id,
                        error,
                        ..
                    } => {
                        inner.log_callback.log(
                            LogLevel::Debug,
                            format!(
                                "chain-connect-attempt-failed; peer_id={}; chain={}; error={}",
                                peer_id, inner.chains[&chain_id].log_name, error
                            ),
                        );

                        // Note that peer doesn't necessarily have an out slot, as this event
                        // might happen as a result of an inbound gossip connection.
                        if inner.network.gossip_remove_desired(
                            chain_id,
                            &peer_id,
                            service::GossipKind::ConsensusTransactions,
                        ) {
                            inner.log_callback.log(
                                LogLevel::Debug,
                                format!(
                                    "slot-unassigned; peer_id={}; chain={}",
                                    peer_id, inner.chains[&chain_id].log_name
                                ),
                            );
                        }

                        if let service::GossipConnectError::GenesisMismatch { .. } = error {
                            inner
                                .peering_strategy
                                .unassign_slot_and_remove_chain_peer(&chain_id, &peer_id);
                        } else {
                            inner.peering_strategy.unassign_slot_and_ban(
                                &chain_id,
                                &peer_id,
                                Instant::now() + Duration::from_secs(15),
                            );
                        }

                        inner.process_network_service_events = true;
                    }
                    service::Event::GossipInDesired {
                        chain_id,
                        peer_id,
                        kind: service::GossipKind::ConsensusTransactions,
                    } => {
                        // TODO: log this
                        // TODO: arbitrary constant
                        // The networking state machine guarantees that `GossipInDesired`
                        // can't happen if we are already opening an out slot, which we do
                        // immediately.
                        // TODO: add debug_assert! ^
                        if inner
                            .network
                            .opened_gossip_undesired_by_chain(chain_id)
                            .count()
                            < 25
                        {
                            inner
                                .network
                                .gossip_open(
                                    chain_id,
                                    &peer_id,
                                    service::GossipKind::ConsensusTransactions,
                                )
                                .unwrap();
                        } else {
                            inner
                                .network
                                .gossip_close(
                                    chain_id,
                                    &peer_id,
                                    service::GossipKind::ConsensusTransactions,
                                )
                                .unwrap();
                        }
                    }
                    service::Event::GossipInDesiredCancel { .. } => {
                        // All `GossipInDesired` are immediately accepted or rejected, meaning
                        // that this event can't happen.
                        unreachable!()
                    }
                    service::Event::RequestResult {
                        substream_id,
                        response: service::RequestResult::Blocks(response),
                    } => {
                        let _ = inner
                            .blocks_requests
                            .remove(&substream_id)
                            .unwrap()
                            .send(response.map_err(BlocksRequestError::Request));
                    }
                    service::Event::RequestResult {
                        substream_id,
                        response: service::RequestResult::KademliaFindNode(Ok(nodes)),
                    } => {
                        let chain_id = inner
                            .kademlia_find_nodes_requests
                            .remove(&substream_id)
                            .unwrap();

                        // TODO: very verbose display
                        inner.log_callback.log(
                            LogLevel::Debug,
                            format!(
                                "discovered; chain={}; nodes={:?}",
                                inner.chains[&chain_id].log_name, nodes
                            ),
                        );

                        for (peer_id, addrs) in nodes {
                            let mut valid_addrs = Vec::with_capacity(addrs.len());
                            for addr in addrs {
                                match Multiaddr::try_from(addr) {
                                    Ok(a) => valid_addrs.push(a),
                                    Err(err) => {
                                        inner.log_callback.log(
                                            LogLevel::Debug,
                                            format!(
                                                "discovery-invalid-address; addr={}",
                                                hex::encode(&err.addr)
                                            ),
                                        );
                                        continue;
                                    }
                                }
                            }

                            if !valid_addrs.is_empty() {
                                inner
                                    .peering_strategy
                                    .insert_chain_peer(chain_id, peer_id.clone());
                            }

                            for addr in valid_addrs {
                                inner
                                    .peering_strategy
                                    .insert_address(&peer_id, addr.into_vec());
                            }
                        }
                    }
                    service::Event::RequestResult {
                        substream_id,
                        response: service::RequestResult::KademliaFindNode(Err(error)),
                    } => {
                        let chain_id = inner
                            .kademlia_find_nodes_requests
                            .remove(&substream_id)
                            .unwrap();
                        inner.log_callback.log(
                            LogLevel::Debug,
                            format!(
                                "discovery-error; chain={}; error={}",
                                inner.chains[&chain_id].log_name, error
                            ),
                        );
                    }
                    service::Event::RequestResult { .. } => {
                        // We never start a request of any other kind.
                        unreachable!()
                    }
                    service::Event::RequestInCancel { .. } => {
                        // Requests are answered immediately, and thus cancelling events can't happen.
                        unreachable!()
                    }
                    service::Event::IdentifyRequestIn {
                        peer_id,
                        substream_id,
                    } => {
                        inner.log_callback.log(
                            LogLevel::Debug,
                            format!("identify-request; peer_id={}", peer_id),
                        );
                        inner
                            .network
                            .respond_identify(substream_id, &inner.identify_agent_version);
                    }
                    service::Event::BlocksRequestIn {
                        peer_id,
                        chain_id,
                        config,
                        substream_id,
                    } => {
                        inner.log_callback.log(
                            LogLevel::Debug,
                            format!(
                                "incoming-blocks-request; peer_id={}; chain={}",
                                peer_id, inner.chains[&chain_id].log_name
                            ),
                        );
                        let mut _jaeger_span = inner.jaeger_service.incoming_block_request_span(
                            &inner.local_peer_id,
                            &peer_id,
                            config.desired_count.get(),
                            if let (1, protocol::BlocksRequestConfigStart::Hash(block_hash)) =
                                (config.desired_count.get(), &config.start)
                            {
                                Some(block_hash)
                            } else {
                                None
                            },
                        );

                        // TODO: is it a good idea to await here while the lock is held and freezing the entire networking background task?
                        let response = blocks_request_response(
                            &inner.chains[&chain_id].database,
                            inner.network.block_number_bytes(chain_id),
                            config,
                        )
                        .await;
                        inner.network.respond_blocks(
                            substream_id,
                            match response {
                                Ok(b) => Some(b),
                                Err(error) => {
                                    inner.log_callback.log(
                                        LogLevel::Warn,
                                        format!("incoming-blocks-request-error; error={}", error),
                                    );
                                    None
                                }
                            },
                        );
                    }
                    service::Event::GrandpaNeighborPacket {
                        chain_id,
                        peer_id,
                        state,
                    } => {
                        inner.log_callback.log(LogLevel::Debug, format!(
                            "grandpa-neighbor-packet; peer_id={}; chain={}; round_number={}; set_id={}; commit_finalized_height={}",
                            peer_id,
                            inner.chains[&chain_id].log_name,
                            state.round_number,
                            state.set_id,
                            state.commit_finalized_height,
                        ));
                        // TODO: report to the sync state machine
                    }
                    service::Event::GrandpaCommitMessage {
                        chain_id,
                        peer_id,
                        message,
                    } => {
                        inner.log_callback.log(
                            LogLevel::Debug,
                            format!(
                                "grandpa-commit-message; peer_id={}; chain={}; target_hash={}",
                                peer_id,
                                inner.chains[&chain_id].log_name,
                                HashDisplay(message.decode().message.target_hash),
                            ),
                        );
                    }
                    service::Event::ProtocolError { peer_id, error } => {
                        inner.log_callback.log(
                            LogLevel::Warn,
                            format!("protocol-error; peer_id={}; error={}", peer_id, error),
                        );
                        inner.peering_strategy.unassign_slots_and_ban(
                            &peer_id,
                            Instant::now() + Duration::from_secs(5),
                        );
                        // TODO: log chain names?
                        inner.log_callback.log(
                            LogLevel::Debug,
                            format!(
                                "all-slots-unassigned; reason=no-address; peer_id={}",
                                peer_id
                            ),
                        );
                        inner.process_network_service_events = true;
                    }
                }
            };

            // Dispatch the event to the various senders.
            if let Some(event) = event {
                // Continue processing events.
                inner.process_network_service_events = true;

                // We check this before generating an event.
                let either::Left(mut event_senders) = inner.event_senders else {
                    unreachable!()
                };

                inner.event_senders = either::Right(Box::pin(async move {
                    // This little `if` avoids having to do `event.clone()` if we don't have to.
                    if event_senders.len() == 1 {
                        let _ = event_senders[0].send(event).await;
                    } else {
                        for sender in event_senders.iter_mut() {
                            // For simplicity we don't get rid of closed senders because senders
                            // aren't supposed to close, and that leaving closed senders in the
                            // list doesn't have any consequence other than one extra iteration
                            // every time.
                            let _ = sender.send(event.clone()).await;
                        }
                    }

                    event_senders
                }));
            }

            // TODO: doc
            for chain_id in inner.chains.keys() {
                loop {
                    // TODO: 25 is an arbitrary constant, make configurable
                    if inner
                        .network
                        .gossip_desired_num(*chain_id, service::GossipKind::ConsensusTransactions)
                        >= 25
                    {
                        break;
                    }

                    let peer_id = match inner.peering_strategy.pick_assignable_peer(chain_id, &Instant::now()) {
                        basic_peering_strategy::AssignablePeer::Assignable(peer_id) => {
                            peer_id.clone()
                        }
                        basic_peering_strategy::AssignablePeer::AllPeersBanned { .. }  // TODO: handle `AllPeersBanned` by waking up when a ban expires
                        | basic_peering_strategy::AssignablePeer::NoPeer => break,
                    };

                    inner.peering_strategy.assign_slot(&chain_id, &peer_id);

                    inner.log_callback.log(
                        LogLevel::Debug,
                        format!(
                            "slot-assigned; peer_id={}; chain={}",
                            peer_id, inner.chains[chain_id].log_name
                        ),
                    );

                    inner.network.gossip_insert_desired(
                        *chain_id,
                        peer_id,
                        service::GossipKind::ConsensusTransactions,
                    );
                }
            }

            // The networking service contains a list of connections that should be opened.
            // Grab this list and start opening a connection for each.
            // TODO: restore the rate limiting for connections openings
            loop {
                if inner.num_pending_out_attempts >= 16 {
                    // TODO: constant
                    break;
                }

                let peer_id = match inner.network.unconnected_desired().next() {
                    Some(p) => p.clone(),
                    None => break,
                };

                inner.num_pending_out_attempts += 1;

                let Some(multiaddr) = inner.peering_strategy.addr_to_connected(&peer_id) else {
                    // There is no address for that peer in the address book.
                    inner.network.gossip_remove_desired_all(
                        &peer_id,
                        service::GossipKind::ConsensusTransactions,
                    );
                    inner
                        .peering_strategy
                        .unassign_slots_and_ban(&peer_id, Instant::now() + Duration::from_secs(10));
                    // TODO: log chain names?
                    inner.log_callback.log(
                        LogLevel::Debug,
                        format!(
                            "all-slots-unassigned; reason=no-address; peer_id={}",
                            peer_id
                        ),
                    );
                    continue;
                };

                let multiaddr = match multiaddr::Multiaddr::try_from(multiaddr.to_owned()) {
                    Ok(a) => a,
                    Err(multiaddr::FromVecError { addr }) => {
                        // Address is in an invalid format.
                        inner.log_callback.log(
                            LogLevel::Debug,
                            format!("invalid-address; peer_id={}; address={:?}", peer_id, addr),
                        );
                        let _was_in = inner.peering_strategy.remove_address(&peer_id, &addr);
                        debug_assert!(_was_in);
                        continue;
                    }
                };

                // Convert the `multiaddr` (typically of the form `/ip4/a.b.c.d/tcp/d`) into
                // a `Future<dyn Output = Result<TcpStream, ...>>`.
                let socket = match tasks::multiaddr_to_socket(&multiaddr) {
                    Ok(socket) => socket,
                    Err(_) => {
                        // Address is in an invalid format or isn't supported.
                        inner.log_callback.log(
                            LogLevel::Debug,
                            format!(
                                "invalid-address; peer_id={}; address={}",
                                peer_id, multiaddr
                            ),
                        );
                        let _was_in = inner
                            .peering_strategy
                            .remove_address(&peer_id, multiaddr.as_ref());
                        debug_assert!(_was_in);
                        continue;
                    }
                };

                let (connection_id, connection_task) = inner.network.add_single_stream_connection(
                    Instant::now(),
                    service::SingleStreamHandshakeKind::MultistreamSelectNoiseYamux {
                        is_initiator: true,
                    },
                    multiaddr.clone().into_vec(),
                    Some(peer_id.clone()),
                );

                let (tx, rx) = channel::bounded(16); // TODO: ?!
                inner.active_connections.insert(connection_id, tx);

                // Handle the connection in a separate task.
                (inner.tasks_executor)(Box::pin(tasks::connection_task(
                    inner.log_callback.clone(),
                    multiaddr.to_string(),
                    socket,
                    connection_id,
                    connection_task,
                    rx,
                    inner.to_background_tx.clone(),
                )));

                inner.process_network_service_events = true;
            }
        }

        // TODO: doc
        loop {
            let Some((peer_id, chain_id)) = inner
                .network
                .connected_unopened_gossip_desired()
                .next()
                .map(|(peer_id, chain_id, _)| (peer_id.clone(), chain_id))
            else {
                break;
            };

            inner
                .network
                .gossip_open(
                    chain_id,
                    &peer_id,
                    service::GossipKind::ConsensusTransactions,
                )
                .unwrap();

            inner.log_callback.log(
                LogLevel::Debug,
                format!(
                    "gossip-open; peer_id={}; chain={}",
                    peer_id, &inner.chains[&chain_id].log_name
                ),
            );
        }

        let message = {
            let foreground_msg = async { Some(inner.to_background_rx.next().await) };
            let sending_done = async {
                if let either::Right(sending) = &mut inner.event_senders {
                    let event_senders = sending.await;
                    inner.event_senders = either::Left(event_senders);
                    None
                } else {
                    future::pending().await
                }
            };

            match foreground_msg.or(sending_done).await {
                Some(msg) => msg.unwrap(),
                None => continue,
            }
        };

        match message {
            ToBackground::FromConnectionTask {
                connection_id,
                opaque_message,
                connection_now_dead,
            } => {
                if let Some(opaque_message) = opaque_message {
                    inner
                        .network
                        .inject_connection_message(connection_id, opaque_message);
                }

                // TODO: it should be indicated by the coordinator when a connection dies
                if connection_now_dead {
                    let _was_in = inner.active_connections.remove(&connection_id);
                    debug_assert!(_was_in.is_some());
                }

                inner.process_network_service_events = true;
            }

            ToBackground::IncomingConnection {
                socket,
                multiaddr,
                when_accepted,
            } => {
                let (connection_id, connection_task) = inner.network.add_single_stream_connection(
                    when_accepted,
                    service::SingleStreamHandshakeKind::MultistreamSelectNoiseYamux {
                        is_initiator: false,
                    },
                    multiaddr.clone().into_vec(),
                    None,
                );

                let (tx, rx) = channel::bounded(16); // TODO: ?!
                inner.active_connections.insert(connection_id, tx);

                (inner.tasks_executor)(Box::pin(tasks::connection_task(
                    inner.log_callback.clone(),
                    multiaddr.to_string(),
                    async move { Ok(socket) },
                    connection_id,
                    connection_task,
                    rx,
                    inner.to_background_tx.clone(),
                )));

                inner.process_network_service_events = true;
            }

            ToBackground::StartKademliaDiscoveries { when_done } => {
                for chain_id in inner.chains.keys() {
                    let random_peer_id =
                        PeerId::from_public_key(&peer_id::PublicKey::Ed25519(rand::random()));

                    // TODO: select target closest to the random peer instead
                    let target = inner
                        .network
                        .gossip_connected_peers(
                            *chain_id,
                            service::GossipKind::ConsensusTransactions,
                        )
                        .next()
                        .map(|p| p.clone());

                    if let Some(target) = target {
                        let substream_id = match inner.network.start_kademlia_find_node_request(
                            &target,
                            *chain_id,
                            &random_peer_id,
                            Duration::from_secs(20),
                        ) {
                            Ok(s) => s,
                            Err(service::StartRequestError::NoConnection) => unreachable!(),
                        };

                        let _prev_value = inner
                            .kademlia_find_nodes_requests
                            .insert(substream_id, *chain_id);
                        debug_assert!(_prev_value.is_none());
                    } else {
                        // TODO: log message
                    }
                }

                let _ = when_done.send(());

                inner.process_network_service_events = true;
            }

            ToBackground::ForegroundShutdown => {
                // TODO: do a clean shutdown of all the connections
                return;
            }

            ToBackground::ForegroundAnnounceBlock {
                target,
                chain_id,
                scale_encoded_header,
                is_best,
                result_tx,
            } => {
                let _ = result_tx.send(inner.network.gossip_send_block_announce(
                    &target,
                    chain_id,
                    &scale_encoded_header,
                    is_best,
                ));
            }
            ToBackground::ForegroundSetLocalBestBlock {
                chain_id,
                best_hash,
                best_number,
            } => {
                inner
                    .network
                    .set_chain_local_best_block(chain_id, best_hash, best_number);
            }
            ToBackground::ForegroundBlocksRequest {
                target,
                chain_id,
                config,
                result_tx,
            } => {
                match inner.network.start_blocks_request(
                    &target,
                    chain_id,
                    config,
                    Duration::from_secs(12),
                ) {
                    Ok(request_id) => {
                        // TODO: somehow cancel the request if the `rx` is dropped?
                        inner.blocks_requests.insert(request_id, result_tx);
                    }
                    Err(service::StartRequestError::NoConnection) => {
                        let _ = result_tx.send(Err(BlocksRequestError::NoConnection));
                    }
                }
            }
            ToBackground::ForegroundGetNumConnections { result_tx } => {
                let _ = result_tx.send(inner.network.num_connections());
            }
            ToBackground::ForegroundGetNumPeers {
                chain_id,
                result_tx,
            } => {
                // TODO: optimize?
                let _ = result_tx.send(
                    inner
                        .network
                        .gossip_connected_peers(
                            chain_id,
                            service::GossipKind::ConsensusTransactions,
                        )
                        .count(),
                );
            }
            ToBackground::ForegroundGetNumTotalPeers { result_tx } => {
                // TODO: optimize?
                let total = inner
                    .chains
                    .keys()
                    .map(|chain_id| {
                        inner
                            .network
                            .gossip_connected_peers(
                                *chain_id,
                                service::GossipKind::ConsensusTransactions,
                            )
                            .count()
                    })
                    .sum();
                let _ = result_tx.send(total);
            }
        }
    }
}

/// Builds the response to a block request by reading from the given database.
async fn blocks_request_response(
    database: &database_thread::DatabaseThread,
    block_number_bytes: usize,
    config: protocol::BlocksRequestConfig,
) -> Result<Vec<protocol::BlockData>, full_sqlite::CorruptedError> {
    database
        .with_database(move |database| {
            let num_blocks = cmp::min(
                usize::try_from(config.desired_count.get()).unwrap_or(usize::max_value()),
                128,
            );

            let mut output = Vec::with_capacity(num_blocks);
            let mut next_block = config.start;

            loop {
                if output.len() >= num_blocks {
                    break;
                }

                let hash = match next_block {
                    protocol::BlocksRequestConfigStart::Hash(hash) => hash,
                    protocol::BlocksRequestConfigStart::Number(number) => {
                        // TODO: naive block selection ; should choose the best chain instead
                        match database.block_hash_by_number(number)?.next() {
                            Some(h) => h,
                            None => break,
                        }
                    }
                };

                let header = match database.block_scale_encoded_header(&hash)? {
                    Some(h) => h,
                    None => break,
                };

                next_block = {
                    let decoded = header::decode(&header, block_number_bytes).unwrap();
                    match config.direction {
                        protocol::BlocksRequestDirection::Ascending => {
                            // TODO: right now, since we don't necessarily pick the best chain in `block_hash_by_number`, it is possible that the next block doesn't have the current block as parent
                            protocol::BlocksRequestConfigStart::Number(decoded.number + 1)
                        }
                        protocol::BlocksRequestDirection::Descending => {
                            protocol::BlocksRequestConfigStart::Hash(*decoded.parent_hash)
                        }
                    }
                };

                output.push(protocol::BlockData {
                    hash,
                    header: if config.fields.header {
                        Some(header)
                    } else {
                        None
                    },
                    body: if config.fields.body {
                        Some(match database.block_extrinsics(&hash)? {
                            Some(body) => body.collect(),
                            None => break,
                        })
                    } else {
                        None
                    },
                    justifications: if config.fields.justifications {
                        // TODO: justifications aren't saved in database at the moment
                        Some(Vec::new())
                    } else {
                        None
                    },
                });
            }

            Ok(output)
        })
        .await
}
